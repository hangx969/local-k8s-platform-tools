
## Install Prometheus Operator CRDs
##
crds:
  enabled: true
  ## The CRD upgrade job mitigates the limitation of helm not being able to upgrade CRDs.
  ## The job will apply the CRDs to the cluster before the operator is deployed, using helm hooks.
  ## It deploy a corresponding clusterrole, clusterrolebinding and serviceaccount to apply the CRDs.
  ## This feature is in preview, off by default and may change in the future.
  upgradeJob:
    enabled: false
    forceConflicts: true
    image:
      busybox:
        registry: docker.io
        repository: busybox
        tag: "1.28"
        sha: ""
        pullPolicy: IfNotPresent
      kubectl:
        registry: docker.io
        repository: bitnami/kubectl # this image repo does not exist AT ALL!!!
        tag: ""  # defaults to the Kubernetes version
        sha: ""
        pullPolicy: IfNotPresent

defaultRules:
  ## Prefix for runbook URLs. Use this to override the first part of the runbookURLs that is common to all rules.
  runbookUrl: "https://runbooks.prometheus-operator.dev/runbooks"
  node:
    fsSelector: 'fstype!=""'

## Configuration for alertmanager
## ref: https://prometheus.io/docs/alerting/alertmanager/
##
alertmanager:
  enabled: true
  config:
    global:
      resolve_timeout: 5m
      slack_api_url: https://slack.com/api/chat.postMessage
      http_config:
        authorization:
          credentials_file: ''
    inhibit_rules:
      - source_matchers:
          - 'severity = critical'
        target_matchers:
          - 'severity =~ warning|info'
        equal:
          - 'namespace'
          - 'alertname'
      - source_matchers:
          - 'severity = warning'
        target_matchers:
          - 'severity = info'
        equal:
          - 'namespace'
          - 'alertname'
      - source_matchers:
          - 'alertname = InfoInhibitor'
        target_matchers:
          - 'severity = info'
        equal:
          - 'namespace'
      - target_matchers:
          - 'alertname = InfoInhibitor'
    route:
      receiver: 'null'
      routes:
      - group_by: ['alertname']
        matchers:
          - alertname = "CriticalVulnerability"
          - namespace =~ "apicurio-schema-registry|cert-manager|external-dns|external-secrets|gatekeeper-system|ingress-nginx|kafka|kafka-ui|kube-system|monitoring|oauth2-proxy|olm|operators|trivy-system|kyverno|log-reader"
        receiver: platform-slack-info
        group_wait: 30s
        # if new alert is added to the group, we wait this long before re-firing the grouped alert again
        group_interval: 24h
        repeat_interval: 120h
        continue: false
      - group_by: ['namespace']
        group_wait: 30s
        group_interval: 5m
        repeat_interval: 24h
        receiver: 'null'
        routes:
        - receiver: 'null'
          matchers:
            - alertname =~ "InfoInhibitor|Watchdog"
        # quota alerts are not compatible with capsule copying tenant quotas for each namespaces' resourcequota
        - receiver: 'null'
          matchers:
            - alertname =~ "KubeCPUQuotaOvercommit|KubeMemoryQuotaOvercommit"
        # not compatible with AKS autoscaling. We have our own alerts
        - receiver: 'platform-slack-info'
          matchers:
            - alertname =~ "KubeCPUOvercommit|KubeMemoryOvercommit|HighCPURequest|HighCPUOverCommit|HighIOWait|HighKernelProcessesTime|NodeSystemSaturation|NodeDiskIOSaturation"
        # does not matter if it's throttled; we don't control it anyway
        - receiver: 'platform-slack-info'
          matchers:
            - alertname =~ "CPUThrottlingHigh"
            - container =~ "microsoft-defender-low-level-collector"
        - receiver: platform-slack
          group_by:
          - alertname
          matchers:
          - namespace = ""
        - receiver: platform-slack
          group_by:
          - namespace
          - alertname
          matchers:
          # 'monitoring' namespace is handled separately
          - namespace =~ "apicurio-schema-registry|cert-manager|external-dns|external-secrets|gatekeeper-system|ingress-nginx|kafka|kafka-ui|kube-system|oauth2-proxy|olm|operators|trivy-system|kyverno|log-reader|monitoring"
          continue: true
    receivers:
    - name: 'null'
    - name: platform-slack
      slack_configs:
        - channel: ''
          send_resolved: true
          title: '{{ template "slack.local.title" . }}'
          text: '{{ template "slack.local.text" . }}'
    - name: platform-slack-info
      slack_configs:
        - channel: ''
          send_resolved: true
          title: '{{ template "slack.local.title" . }}'
          text: '{{ template "slack.local.text" . }}'
    templates:
    - '/etc/alertmanager/config/*.tmpl'

  templateFiles:
    default_title.tmpl: |-
      {{ define "slack.local.title" }}
      {{ .CommonLabels.alertname }} - {{ .Alerts.Firing | len }} in {{ .CommonLabels.namespace }}
      {{ end }}
    default_text.tmpl: |-
      {{ define "slack.local.text" }}
      {{ range .Alerts }}
      `{{ .Labels.severity }}` - {{ .Annotations.summary }}
      *Description:* {{ .Annotations.description }}
      {{ if match `^http.+` .GeneratorURL }}*Graph:* <{{ .GeneratorURL }}|:chart_with_upwards_trend:> {{ end }}{{ if .Annotations.runbook }}*Runbook:* <{{ .Annotations.runbook }}|:spiral_note_pad:>{{ end }}
      *Details:*
        {{ range .Labels.SortedPairs }} • *{{ .Name }}:* `{{ .Value }}`
        {{ end }}
      {{ end }}
      {{ end }}

  ingress:
    enabled: true
    ingressClassName: nginx-default
    annotations:
      nginx.ingress.kubernetes.io/auth-url: "http://oauth2-proxy.oauth2-proxy.svc.cluster.local/oauth2/auth"
      nginx.ingress.kubernetes.io/auth-signin: "https://oauth2proxy.hanxux.local/oauth2/start?rd=https%3A%2F%2Falertmanager.hanxux.local"
    hosts:
      - alertmanager.hanxux.local
    ## Paths to use for ingress rules - one path should match the alertmanagerSpec.routePrefix
    ##
    pathType: Prefix
    paths: ["/"]
    tls:
    - secretName: alertmanager-tls-cert-secret
      hosts:
      - alertmanager.hanxux.local

  alertmanagerSpec:
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: sc-nfs
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 1Gi

grafana:
  enabled: true
  resources:
    limits:
      cpu: 600m
      memory: 1Gi
    requests:
      cpu: 50m
      memory: 100Mi
  image:
    repository: grafana/grafana
  adminPassword: '' # should be overwritten in workflow
  ingress:
    enabled: true
    ingressClassName: nginx-default
    annotations:
      nginx.ingress.kubernetes.io/auth-url: "http://oauth2-proxy.oauth2-proxy.svc.cluster.local/oauth2/auth"
      nginx.ingress.kubernetes.io/auth-signin: "https://oauth2proxy.hanxux.local/oauth2/start?rd=https%3A%2F%2Fgrafana.hanxux.local"
    hosts:
      - grafana.hanxux.local
    pathType: Prefix
    paths:
      - /
    tls:
      - secretName: grafana-tls-cert-secret
        hosts:
        - grafana.hanxux.local
  sidecar:
    resources:
      requests:
        cpu: 50m
        memory: 180Mi
      limits:
        cpu: 900m
        memory: 500Mi
  additionalDataSources:
    - name: Tempo
      type: tempo
      uid: tempo
      url: http://tempo.monitoring.svc:3100
      access: proxy
      version: 1
      jsonData:
        httpMethod: GET
        serviceMap:
          datasourceUid: 'prometheus'
        tracesToLogsV2: # https://grafana.com/docs/grafana/next/datasources/tempo/#trace-to-logs
          datasourceUid: 'Loki'
          spanStartTimeShift: '-5m'
          spanEndTimeShift: '5m'
          tags: [{ key: 'service.name', value: 'app' }, { key: 'host.name', value: 'pod' }]
    - name: Loki
      type: loki
      url: http://loki.monitoring.svc:3100
      access: proxy

  persistence:
    enabled: true
    type: sts
    storageClassName: "sc-nfs"
    accessModes:
      - ReadWriteOnce
    size: 2Gi
    finalizers:
      - kubernetes.io/pvc-protection

## Component scraping the kube controller manager
kubeControllerManager:
  enabled: true
  endpoints:
  - 192.168.40.180
  service:
    enabled: true
    port: 10257
    targetPort: 10257
    selector:
      component: kube-controller-manager
  serviceMonitor:
    enabled: true
    https: true
    insecureSkipVerify: true
    serverName: localhost
    interval: 30s
    metricRelabelings: []
    relabelings: []

kubeScheduler:
  enabled: true
  endpoints:
  - 192.168.40.180
  service:
    enabled: true
    port: 10259
    targetPort: 10259
    selector:
      component: kube-scheduler
  serviceMonitor:
    enabled: true
    https: true
    insecureSkipVerify: true
    serverName: localhost
    interval: 30s
    metricRelabelings: []
    relabelings: []

kubeEtcd:
  enabled: true
  endpoints:
  - 192.168.40.180
  service:
    enabled: true
    port: 2381
    targetPort: 2381
    selector:
      component: etcd
  serviceMonitor:
    enabled: true
    https: true
    insecureSkipVerify: true
    serverName: localhost
    interval: 30s
    metricRelabelings: []
    relabelings: []

kubeProxy:
  enabled: true
  service:
    enabled: true
    port: 10249
    targetPort: 10249
    selector:
      k8s-app: kube-proxy
  serviceMonitor:
    enabled: true
    https: false
    interval: 30s
    metricRelabelings: []
    relabelings: []

kube-state-metrics:
  resources:
    requests:
      cpu: 10m
      memory: 80Mi
    limits:
      cpu: 200m
      memory: 500Mi

prometheus-node-exporter:
  resources:
    requests:
      cpu: 20m
      memory: 30Mi
    limits:
      memory: 50Mi
  service:
    labels:
      jobLabel: node-exporter
  prometheus:
    monitor:
      relabelings:
      - sourceLabels: [__meta_kubernetes_pod_node_name]
        separator: ;
        regex: ^(.*)$
        targetLabel: node
        replacement: $1
        action: replace

  ## Admission webhook support for PrometheusRules resources added in Prometheus Operator 0.30 can be enabled to prevent incorrectly formatted
  ## rules from making their way into prometheus and potentially preventing the container from starting
  admissionWebhooks:
    ## Valid values: Fail, Ignore, IgnoreOnInstallOnly
    ## IgnoreOnInstallOnly - If Release.IsInstall returns "true", set "Ignore" otherwise "Fail"
    failurePolicy: Ignore
    patch:
      enabled: false
      image:
        registry: registry.cn-hangzhou.aliyuncs.com/google_containers
        # repository: kube-webhook-certgen
        # tag: v20221220-controller-v1.5.1-58-g787ea74b6
        # sha: ""
        pullPolicy: IfNotPresent

  prometheusConfigReloader:
    resources:
      requests:
        cpu: 200m
        memory: 50Mi
      limits:
        cpu: 200m
        memory: 50Mi

prometheus:
  ingress:
    enabled: true
    ingressClassName: nginx-default
    annotations: {}
    labels: {}
    hosts:
      - prometheus.hanxux.local
    pathType: Prefix
    paths:
      - /
    tls: []
  prometheusSpec:
    enableRemoteWriteReceiver: true
    ## If true, a nil or {} value for prometheus.prometheusSpec.ruleSelector will cause the
    ## prometheus resource to be created with selectors based on values in the helm deployment,
    ## which will also match the PrometheusRule resources created
    ##
    ruleSelectorNilUsesHelmValues: false

    ## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the
    ## prometheus resource to be created with selectors based on values in the helm deployment,
    ## which will also match the servicemonitors created
    ##
    serviceMonitorSelectorNilUsesHelmValues: false

    ## If true, a nil or {} value for prometheus.prometheusSpec.podMonitorSelector will cause the
    ## prometheus resource to be created with selectors based on values in the helm deployment,
    ## which will also match the podmonitors created
    ##
    podMonitorSelectorNilUsesHelmValues: false

    retention: 7d

    ## Enable compression of the write-ahead log using Snappy.
    ##
    walCompression: false
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: sc-nfs
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 1Gi
    ## Secrets is a list of Secrets in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.
    ## The Secrets are mounted into /etc/prometheus/secrets/. Secrets changes after initial creation of a Prometheus object are not
    ## reflected in the running Pods. To change the secrets mounted into the Prometheus Pods, the object must be deleted and recreated
    ## with the new list of secrets.
    ##
    secrets:
    - etcd-ssl # 挂载etcd的证书，用于获取etcd metrics数据
    - controller-manager-ssl # 挂载controller-manager的证书，用于获取metrics数据
